{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07394a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class TransformerJacobianAnalyzer:\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"Initialize with a Hugging Face transformer model\"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def prepare_input(self, text: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Tokenize and prepare input for the model\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        return {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "    def compute_jacobians(self, inputs: Dict[str, torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \"\"\"Compute Jacobians of attention and MLP functions for all layers\"\"\"\n",
    "        input_ids = inputs['input_ids']\n",
    "        seq_len = input_ids.size(1)\n",
    "\n",
    "        # Get initial embeddings\n",
    "        embeddings = self.model.wte(input_ids) + self.model.wpe(torch.arange(seq_len, device=self.device))\n",
    "\n",
    "        def attention_function(emb: torch.Tensor, layer) -> torch.Tensor:\n",
    "            \"\"\"Attention function for a single layer\"\"\"\n",
    "            return layer.attn(layer.ln_1(emb))[0]\n",
    "\n",
    "        def compute_jacobian_attn(emb, layer):\n",
    "            emb_before, embed_last = emb[:-1], emb[-1]\n",
    "            def attn_i(emb_i):\n",
    "                hidden_states = torch.cat([emb_before, emb_i[None, ...]], dim=0)\n",
    "                return layer.attn(layer.ln_1(hidden_states))[0]\n",
    "\n",
    "            return torch.autograd.functional.jacobian(attn_i, embed_last)\n",
    "\n",
    "\n",
    "        def feedforward_function(emb: torch.Tensor, layer) -> torch.Tensor:\n",
    "            \"\"\"MLP/feedforward function for a single layer\"\"\"\n",
    "            return layer.mlp(layer.ln_2(emb))\n",
    "\n",
    "        def compute_jacobian_feedforward(emb, layer):\n",
    "            # only consider the last token\n",
    "            emb_i = emb[-1]\n",
    "            def mlp(emb_i):\n",
    "                return layer.mlp(layer.ln_2(emb_i))\n",
    "\n",
    "            return torch.autograd.functional.jacobian(mlp, emb_i)\n",
    "\n",
    "        hidden_states = embeddings.requires_grad_(True)\n",
    "        jacobian_attns, jacobian_mlps = [], []\n",
    "\n",
    "        print(f\"Computing Jacobians for {len(self.model.h)} layers...\")\n",
    "\n",
    "        for i, layer in enumerate(self.model.h):\n",
    "            print(f\"Processing layer {i+1}/{len(self.model.h)}\")\n",
    "\n",
    "            # Compute attention Jacobian\n",
    "            try:\n",
    "                attn_fn = partial(attention_function, layer=layer)\n",
    "                jacobian_attn = compute_jacobian_attn(hidden_states, layer)\n",
    "                jacobian_attns.append(jacobian_attn.detach().cpu())\n",
    "\n",
    "                # Apply attention with residual connection\n",
    "                attn_output = attn_fn(hidden_states)\n",
    "                hidden_states = hidden_states + attn_output\n",
    "                # hidden_states = hidden_states.detach().requires_grad_(True)\n",
    "\n",
    "                # Compute MLP Jacobian\n",
    "                mlp_fn = partial(feedforward_function, layer=layer)\n",
    "                jacobian_mlp = compute_jacobian_feedforward(hidden_states, layer)\n",
    "                jacobian_mlps.append(jacobian_mlp.detach().cpu())\n",
    "\n",
    "                # Apply MLP with residual connection\n",
    "                mlp_output = mlp_fn(hidden_states)\n",
    "                hidden_states = hidden_states + mlp_output\n",
    "                # hidden_states = hidden_states.detach().requires_grad_(True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing Jacobian for layer {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return jacobian_attns, jacobian_mlps\n",
    "\n",
    "    def compute_eigenvalues(self, jacobian: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute eigenvalues and eigenvectors from Jacobian matrix\"\"\"\n",
    "        # Convert to numpy\n",
    "        jac_np = jacobian.cpu().numpy()\n",
    "\n",
    "        # Reshape to 2D if needed (flatten spatial dimensions)\n",
    "        original_shape = jac_np.shape\n",
    "        if len(original_shape) > 2:\n",
    "            # Reshape to (output_features, input_features)\n",
    "            batch_size, seq_len, hidden_dim = original_shape[:3]\n",
    "            jac_2d = jac_np.reshape(-1, original_shape[-1])\n",
    "        else:\n",
    "            jac_2d = jac_np\n",
    "\n",
    "        # For non-square matrices, use SVD for stability\n",
    "        if jac_2d.shape[0] != jac_2d.shape[1]:\n",
    "            # Use singular value decomposition\n",
    "            U, s, Vt = np.linalg.svd(jac_2d, full_matrices=False)\n",
    "            eigenvals = s  # Singular values\n",
    "            eigenvecs = Vt.T  # Right singular vectors\n",
    "        else:\n",
    "            # Standard eigendecomposition for square matrices\n",
    "            try:\n",
    "                eigenvals, eigenvecs = np.linalg.eig(jac_2d)\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Fallback to SVD if eigendecomposition fails\n",
    "                U, s, Vt = np.linalg.svd(jac_2d, full_matrices=False)\n",
    "                eigenvals = s\n",
    "                eigenvecs = Vt.T\n",
    "\n",
    "        # Sort by magnitude\n",
    "        idx = np.argsort(np.abs(eigenvals))[::-1]\n",
    "        eigenvals = eigenvals[idx]\n",
    "        eigenvecs = eigenvecs[:, idx]\n",
    "\n",
    "        return eigenvals, eigenvecs\n",
    "\n",
    "    def spectral_analysis(self, jacobian_attns: List[torch.Tensor],\n",
    "                         jacobian_mlps: List[torch.Tensor], prompt) -> Dict:\n",
    "        \"\"\"Perform comprehensive spectral analysis on Jacobians\"\"\"\n",
    "\n",
    "        results = {\n",
    "            'prompt': prompt,\n",
    "            'attention_analysis': [],\n",
    "            'mlp_analysis': [],\n",
    "            'layer_statistics': {}\n",
    "        }\n",
    "\n",
    "        print(\"Performing spectral analysis...\")\n",
    "\n",
    "        for i, (jac_attn, jac_mlp) in enumerate(zip(jacobian_attns, jacobian_mlps)):\n",
    "            print(f\"Analyzing layer {i+1}\")\n",
    "\n",
    "            # Attention analysis\n",
    "            attn_eigenvals, attn_eigenvecs = self.compute_eigenvalues(jac_attn)\n",
    "            attn_analysis = {\n",
    "                'layer': i,\n",
    "                'eigenvalues': attn_eigenvals,\n",
    "                'eigenvectors': attn_eigenvecs,\n",
    "                'spectral_radius': np.max(np.abs(attn_eigenvals)),\n",
    "                'condition_number': np.max(np.abs(attn_eigenvals)) / np.min(np.abs(attn_eigenvals[attn_eigenvals != 0])) if np.any(attn_eigenvals != 0) else np.inf,\n",
    "                'rank_estimate': np.sum(np.abs(attn_eigenvals) > 1e-10),\n",
    "                'dominant_eigenvalue': attn_eigenvals[0],\n",
    "                'eigenvalue_decay': np.abs(attn_eigenvals[1] / attn_eigenvals[0]) if len(attn_eigenvals) > 1 else 0\n",
    "            }\n",
    "            results['attention_analysis'].append(attn_analysis)\n",
    "\n",
    "            # MLP analysis\n",
    "            mlp_eigenvals, mlp_eigenvecs = self.compute_eigenvalues(jac_mlp)\n",
    "            mlp_analysis = {\n",
    "                'layer': i,\n",
    "                'eigenvalues': mlp_eigenvals,\n",
    "                'eigenvectors': mlp_eigenvecs,\n",
    "                'spectral_radius': np.max(np.abs(mlp_eigenvals)),\n",
    "                'condition_number': np.max(np.abs(mlp_eigenvals)) / np.min(np.abs(mlp_eigenvals[mlp_eigenvals != 0])) if np.any(mlp_eigenvals != 0) else np.inf,\n",
    "                'rank_estimate': np.sum(np.abs(mlp_eigenvals) > 1e-10),\n",
    "                'dominant_eigenvalue': mlp_eigenvals[0],\n",
    "                'eigenvalue_decay': np.abs(mlp_eigenvals[1] / mlp_eigenvals[0]) if len(mlp_eigenvals) > 1 else 0\n",
    "            }\n",
    "            results['mlp_analysis'].append(mlp_analysis)\n",
    "\n",
    "        # Overall statistics\n",
    "        attn_spectral_radii = [a['spectral_radius'] for a in results['attention_analysis']]\n",
    "        mlp_spectral_radii = [m['spectral_radius'] for m in results['mlp_analysis']]\n",
    "\n",
    "        results['layer_statistics'] = {\n",
    "            'mean_attn_spectral_radius': np.mean(attn_spectral_radii),\n",
    "            'std_attn_spectral_radius': np.std(attn_spectral_radii),\n",
    "            'mean_mlp_spectral_radius': np.mean(mlp_spectral_radii),\n",
    "            'std_mlp_spectral_radius': np.std(mlp_spectral_radii),\n",
    "            'max_attn_spectral_radius': np.max(attn_spectral_radii),\n",
    "            'max_mlp_spectral_radius': np.max(mlp_spectral_radii)\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_spectral_analysis(self, results: Dict, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot comprehensive spectral analysis results\"\"\"\n",
    "\n",
    "        num_layers = len(results['attention_analysis'])\n",
    "\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        fig.suptitle(f'Spectral Analysis: {self.model_name}, prompt: {results[\"prompt\"]}', fontsize=16)\n",
    "\n",
    "        # Plot 1: Spectral radii across layers\n",
    "        layers = range(num_layers)\n",
    "        attn_radii = [r['spectral_radius'] for r in results['attention_analysis']]\n",
    "        mlp_radii = [r['spectral_radius'] for r in results['mlp_analysis']]\n",
    "\n",
    "        axes[0, 0].plot(layers, attn_radii, 'o-', label='Attention', color='blue')\n",
    "        axes[0, 0].plot(layers, mlp_radii, 's-', label='MLP', color='red')\n",
    "        axes[0, 0].set_xlabel('Layer')\n",
    "        axes[0, 0].set_ylabel('Spectral Radius')\n",
    "        axes[0, 0].set_title('Spectral Radius by Layer')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "\n",
    "        # Plot 2: Condition numbers\n",
    "        attn_cond = [r['condition_number'] for r in results['attention_analysis']]\n",
    "        mlp_cond = [r['condition_number'] for r in results['mlp_analysis']]\n",
    "\n",
    "        axes[0, 1].semilogy(layers, attn_cond, 'o-', label='Attention', color='blue')\n",
    "        axes[0, 1].semilogy(layers, mlp_cond, 's-', label='MLP', color='red')\n",
    "        axes[0, 1].set_xlabel('Layer')\n",
    "        axes[0, 1].set_ylabel('Condition Number (log)')\n",
    "        axes[0, 1].set_title('Condition Numbers by Layer')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "\n",
    "        # Plot 3: Rank estimates\n",
    "        attn_ranks = [r['rank_estimate'] for r in results['attention_analysis']]\n",
    "        # mlp_ranks = [r['rank_estimate'] for r in results['mlp_analysis']]\n",
    "\n",
    "        axes[0, 2].plot(layers, attn_ranks, 'o-', label='Attention', color='blue')\n",
    "        # axes[0, 2].plot(layers, mlp_ranks, 's-', label='MLP', color='red')\n",
    "        axes[0, 2].set_xlabel('Layer')\n",
    "        axes[0, 2].set_ylabel('Estimated Rank')\n",
    "        axes[0, 2].set_title('Rank Estimates by Layer')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True)\n",
    "\n",
    "        # Plot 4: Eigenvalue spectrum for first layer\n",
    "        if results['attention_analysis']:\n",
    "            eigenvals = results['attention_analysis'][0]['eigenvalues'][:50]  # Top 50\n",
    "            axes[1, 0].semilogy(range(len(eigenvals)), np.abs(eigenvals), 'o-')\n",
    "            axes[1, 0].set_xlabel('Eigenvalue Index')\n",
    "            axes[1, 0].set_ylabel('|Eigenvalue| (log)')\n",
    "            axes[1, 0].set_title('Attention Eigenvalue Spectrum (Layer 0)')\n",
    "            axes[1, 0].grid(True)\n",
    "\n",
    "        # Plot 5: MLP eigenvalue spectrum for first layer\n",
    "        if results['mlp_analysis']:\n",
    "            eigenvals = results['mlp_analysis'][0]['eigenvalues'][:50]  # Top 50\n",
    "            axes[1, 1].semilogy(range(len(eigenvals)), np.abs(eigenvals), 'o-', color='red')\n",
    "            axes[1, 1].set_xlabel('Eigenvalue Index')\n",
    "            axes[1, 1].set_ylabel('|Eigenvalue| (log)')\n",
    "            axes[1, 1].set_title('MLP Eigenvalue Spectrum (Layer 0)')\n",
    "            axes[1, 1].grid(True)\n",
    "\n",
    "        # Plot 6: Eigenvalue decay rates\n",
    "        attn_decay = [r['eigenvalue_decay'] for r in results['attention_analysis']]\n",
    "        mlp_decay = [r['eigenvalue_decay'] for r in results['mlp_analysis']]\n",
    "\n",
    "        axes[1, 2].plot(layers, attn_decay, 'o-', label='Attention', color='blue')\n",
    "        axes[1, 2].plot(layers, mlp_decay, 's-', label='MLP', color='red')\n",
    "        axes[1, 2].set_xlabel('Layer')\n",
    "        axes[1, 2].set_ylabel('λ₂/λ₁ Ratio')\n",
    "        axes[1, 2].set_title('Eigenvalue Decay Rate')\n",
    "        axes[1, 2].legend()\n",
    "        axes[1, 2].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85657b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello World!'\n",
    "\n",
    "analyzer = TransformerJacobianAnalyzer(\"gpt2\")\n",
    "inputs = analyzer.prepare_input(prompt)\n",
    "jac_attn, jac_mlp = analyzer.compute_jacobians(inputs)\n",
    "results = analyzer.spectral_analysis(jac_attn, jac_mlp, prompt)\n",
    "analyzer.plot_spectral_analysis(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
